import matplotlib.pyplot as plt
import numpy as np
import random
from fractions import Fraction

#see paper https://arxiv.org/abs/2004.00010

#sample uniformly from range(m)
def sample_uniform(m,rng):
    assert isinstance(m,int)
    assert m>0
    return rng.randrange(m)

#sample from a Bernoulli(p) distribution
#assumes p is a rational number in [0,1]
def sample_bernoulli(p,rng):
    assert isinstance(p,Fraction)
    assert 0 <= p <= 1
    m=sample_uniform(p.denominator,rng)
    if m < p.numerator:
        return 1
    else:
        return 0

#sample from a Bernoulli(exp(-x)) distribution
#assumes x is a rational number in [0,1]
def sample_bernoulli_exp1(x,rng):
    assert isinstance(x,Fraction)
    assert 0 <= x <= 1
    k=1
    while True:
        if sample_bernoulli(x/k,rng)==1:
            k=k+1
        else:
            break
    return k%2

#sample from a Bernoulli(exp(-x)) distribution
#assumes x is a rational number >=0
def sample_bernoulli_exp(x,rng):
    assert isinstance(x,Fraction)
    assert x >= 0
    #Sample floor(x) independent Bernoulli(exp(-1))
    #If all are 1, return Bernoulli(exp(-(x-floor(x))))
    while x>1:
        if sample_bernoulli_exp1(Fraction(1,1),rng)==1:
            x=x-1
        else:
            return 0
    return sample_bernoulli_exp1(x,rng)

#sample from a geometric(1-exp(-x)) distribution
#assumes x is a rational number >= 0
def sample_geometric_exp_slow(x,rng):
    assert isinstance(x,Fraction)
    assert x >= 0
    k=0
    while True:
        if sample_bernoulli_exp(x,rng)==1:
            k=k+1
        else:
            return k

#sample from a geometric(1-exp(-x)) distribution
#assumes x >= 0 rational
def sample_geometric_exp_fast(x,rng):
    assert isinstance(x,Fraction)
    if x==0: return 0 #degenerate case
    assert x>0

    t=x.denominator
    while True:
        u=sample_uniform(t,rng)
        b=sample_bernoulli_exp(Fraction(u,t),rng)
        if b==1:
            break
    v=sample_geometric_exp_slow(Fraction(1,1),rng)
    value = v*t+u
    return value//x.numerator

#sample from a discrete Laplace(scale) distribution
#Returns integer x with Pr[x] = exp(-abs(x)/scale)*(exp(1/scale)-1)/(exp(1/scale)+1)
#p = exp(-1/scale) with scale > 1
#casts scale to Fraction
#assumes scale>=0
def sample_dlaplace(scale,rng=None):
    if rng is None:
        rng = random.SystemRandom()
    scale = Fraction(scale)
    assert scale >= 0
    while True:
        sign=sample_bernoulli(Fraction(1,2),rng)
        magnitude=sample_geometric_exp_fast(1/scale,rng)
        if sign==1 and magnitude==0: continue
        return magnitude*(1-2*sign)

#compute floor(sqrt(x)) exactly
#only requires comparisons between x and integer
def floorsqrt(x):
    assert x >= 0
    #a,b integers
    a=0 #maintain a^2<=x
    b=1 #maintain b^2>x
    while b*b <= x:
        b=2*b #double to get upper bound
    #now do binary search
    while a+1<b:
        c=(a+b)//2 #c=floor((a+b)/2)
        if c*c <= x:
            a=c
        else:
            b=c
    #check nothing funky happened
    #assert isinstance(a,int) #python 3
    #assert isinstance(a,(int,long)) #python 2
    return a

#sample from a discrete Gaussian distribution N_Z(0,sigma2)
#Returns integer x with Pr[x] = exp(-x^2/(2*sigma2))/normalizing_constant(sigma2)
#mean 0 variance ~= sigma2 for large sigma2
#casts sigma2 to Fraction
#assumes sigma2>=0
def sample_dgauss(sigma2,rng=None):
    if rng is None:
        rng = random.SystemRandom()
    sigma2=Fraction(sigma2)
    if sigma2==0: return 0 #degenerate case
    assert sigma2 > 0
    t = floorsqrt(sigma2)+1
    while True:
        candidate = sample_dlaplace(t,rng=rng)
        bias=((abs(candidate)-sigma2/t)**2)/(2*sigma2)
        if sample_bernoulli_exp(bias,rng)==1:
            return candidate

#########################################################################
#DONE That's it! Now some utilities

import math #need this, code below is no longer exact

#Compute the normalizing constant of the discrete gaussian
#i.e. sum_{x in Z} exp(-x^2/2sigma2)
#By Poisson summation formula, this is equivalent to
# sqrt{2*pi*sigma2}*sum_{y in Z} exp(-2*pi^2*sigma2*y^2)
#For small sigma2 the former converges faster
#For large sigma2, the latter converges faster
#crossover at sigma2=1/2*pi
#For intermediate sigma2, this code will compute both and check
def normalizing_constant(sigma2):
    original=None
    poisson=None
    if sigma2<=1:
        original = 0
        x=1000 #summation stops at exp(-x^2/2sigma2)<=exp(-500,000)
        while x>0:
            original = original + math.exp(-x*x/(2.0*sigma2))
            x = x - 1 #sum from small to large for improved accuracy
        original = 2*original + 1 #symmetrize and add x=0
    if sigma2*100 >= 1:
        poisson = 0
        y = 1000 #summation stops at exp(-y^2*2*pi^2*sigma2)<=exp(-190,000)
        while y>0:
            poisson = poisson + math.exp(-math.pi*math.pi*sigma2*2*y*y)
            y = y - 1 #sum from small to large
        poisson = math.sqrt(2*math.pi*sigma2)*(1+2*poisson)
    if poisson is None: return original
    if original is None: return poisson
    #if we have computed both, check equality
    scale = max(1,math.sqrt(2*math.pi*sigma2)) #tight-ish lower bound on constant
    assert -1e-15*scale <= original-poisson <= 1e-15*scale
    #10^-15 is about as much precision as we can expect from double precision floating point numbers
    #64-bit float has 56-bit mantissa 10^-15 ~= 2^-50
    return (original+poisson)/2

#compute the variance of discrete gaussian
#mean is zero, thus:
#var = sum_{x in Z} x^2*exp(-x^2/(2*sigma2)) / normalizing_constant(sigma2)
#By Poisson summation formula, we have equivalent expression:
# variance(sigma2) = sigma2 * (1 - 4*pi^2*sigma2*variance(1/(4*pi^2*sigma2)) )
#See lemma 20 https://arxiv.org/pdf/2004.00010v3.pdf#page=17
#alternative expression converges faster when sigma2 is large
#crossover point (in terms of convergence) is sigma2=1/(2*pi)
#for intermediate values of sigma2, we compute both expressions and check
def variance(sigma2):
    original=None
    poisson=None
    if sigma2<=1: #compute primary expression
        original=0
        x = 1000 #summation stops at exp(-x^2/2sigma2)<=exp(-500,000)
        while x>0: #sum from small to large for improved accuracy
            original = original + x*x*math.exp(-x*x/(2.0*sigma2))
            x=x-1
        original = 2*original/normalizing_constant(sigma2)
    if sigma2*100>=1:
        poisson=0 #we will compute sum_{y in Z} y^2 * exp(-2*pi^2*sigma2*y^2)
        y=1000 #summation stops at exp(-y^2*2*pi^2*sigma2)<=exp(-190,000)
        while y>0: #sum from small to large
            poisson = poisson + y*y*math.exp(-y*y*2*sigma2*math.pi*math.pi)
            y=y-1
        poisson = 2*poisson/normalizing_constant(1/(4*sigma2*math.pi*math.pi))
        #next convert from variance(1/(4*pi^2*sigma2)) to variance(sigma2)
        poisson = sigma2*(1-4*sigma2*poisson*math.pi*math.pi)
    if original is None: return poisson
    if poisson is None: return original
    #if we have computed both check equality
    assert -1e-15*sigma2 <= original-poisson <= 1e-15*sigma2
    return (original+poisson)/2

def discrete_laplacian_comparison(matrix, exponent, coefficients, epsilon, sensitivity):

  real_powered_matrix = np.power(matrix, exponent)
  real_product_vector = np.prod(real_powered_matrix, axis=0)
  real_sum_vector = np.multiply(real_product_vector, coefficients)
  real_sum = np.sum(real_sum_vector)

  n, m = matrix.shape
  discrete_laplacian_matrix = [[sample_dlaplace(sensitivity/epsilon,rng=None) for j in range(m)] for i in range(n)]
  # coefficients and exponents are publicly known values

  error_powered_matrix = real_powered_matrix + discrete_laplacian_matrix
  error_product_vector = np.prod(error_powered_matrix, axis=0)
  error_sum_vector = np.multiply(error_product_vector, coefficients)
  error_sum = np.sum(error_sum_vector)

  if error_sum == np.inf or error_sum == 0:
    if real_sum == np.inf or real_sum == 0:
      ratio = 1
    else: 
      ratio = 0.01/real_sum
  else:
    ratio = real_sum / error_sum

  #if np.isnan(ratio):
    #ratio = 0

  return ratio

def plot_means(x):
  mins = x.min(0)
  maxes = x.max(0)
  means = x.mean(0)
  std = x.std(0)

  new_blue = (0.2, 0.2, 0.8)

  plt.errorbar(["100", "1000", "10000", "100000"], means, std, fmt='ok', ecolor='red', lw=3)
  plt.errorbar(np.arange(4), means, [means - mins, maxes - means],
              fmt='.k', ecolor=new_blue, lw=1)

  plt.xlabel('Mean of users inputs')

def plot_epsilons(x):
  mins = x.min(0)
  maxes = x.max(0)
  means = x.mean(0)
  std = x.std(0)

  new_blue = (0.2, 0.2, 0.8)

  plt.errorbar(["0.01", "0.1", "1", "10"], means, std, fmt='ok', ecolor='red', lw=3)
  plt.errorbar(np.arange(4), means, [means - mins, maxes - means],
              fmt='.k', ecolor=new_blue, lw=1)

  plt.xlabel('Epsilon')

def discrete_laplacian_testing_means(n, m, repetitions, epsilon):
  matrix_ratios = np.array([[0 for _ in range(4)] for _ in range(repetitions)])
  matrix_means = [100, 1000, 10000, 100000]
  matrix_coeffs = [5, 50, 500, 5000]
  matrix_width = [1, 10, 100, 1000]
  for i in range(4):
    mean = matrix_means[i]
    coeff = matrix_coeffs[i]
    width = matrix_width[i]
    for rep in range(repetitions):
      #matrix is a matrix N x M where N stands for the “types of value” and M is the number of users (S-1)
      matrix = np.random.uniform(mean - width, mean + width, size=(n, m))
      #exponent is a matrix N x M that has the publicly known values of e_ij
      exponent = np.random.randint(1, 11, size=(n, m))
      #coefficient is a vector 1 x M that has the publicly known values of c_j
      coefficients = np.random.uniform(coeff * .8, coeff * 1.2, size=(1,m))
      matrix_ratios[rep][i] = discrete_laplacian_comparison(matrix, exponent, coefficients, epsilon, width)

  return matrix_ratios

def discrete_laplacian_testing_epsilons(n, m, repetitions, mean):
  matrix_ratios = np.array([[0 for _ in range(4)] for _ in range(repetitions)])
  matrix_epsilons = matrix_width = [0.01, 0.1, 1, 10]
  coeff = 5
  for i in range(4):
    epsilon = matrix_epsilons[i]
    width = matrix_width[i]
    for rep in range(repetitions):
      #matrix is a matrix N x M where N stands for the “types of value” and M is the number of users (S-1)
      matrix = np.random.uniform(mean - width, mean + width, size=(n, m))
      #exponent is a matrix N x M that has the publicly known values of e_ij
      exponent = np.random.randint(1, 11, size=(n, m))
      #coefficient is a vector 1 x M that has the publicly known values of c_j
      coefficients = np.random.uniform(coeff * .8, coeff * 1.2, size=(1,m))
      matrix_ratios[rep][i] = discrete_laplacian_comparison(matrix, exponent, coefficients, epsilon, width)

  return matrix_ratios

n = 100 # number of users
epsilon = 1
mean = 100
repetitions = 100
#matrix_means = discrete_laplacian_testing_means(n, n, repetitions, epsilon)
#plot_means(matrix_means)
matrix_epsilons = discrete_laplacian_testing_epsilons(n, n, repetitions, mean)
plot_epsilons(matrix_epsilons)
